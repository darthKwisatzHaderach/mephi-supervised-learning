import numpy as np
from matplotlib import pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


x = np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4]).reshape((-1, 1))
y = np.array([16, 9, 4, 1, 0, 1, 4, 9, 16])

model = LinearRegression()
model.fit(x, y)
y_pred = model.predict(x)

plt.scatter(x, y)
plt.plot(x, y_pred, color='red', linewidth=2)
plt.show()

print('Mean Absolute Error:', mean_absolute_error(y, y_pred))
print('Mean Squared Error:', mean_squared_error(y, y_pred))
print('R2 score:', r2_score(y, y_pred))

# Таким образом, линейная регрессия (и любая другая линейная модель) имеет ряд ограничений):
# - Линейная связь — линейная регрессия требует, чтобы связь между независимыми и зависимыми переменными была линейной.
# - Выбросы — также важно проверять наличие выбросов, поскольку линейная регрессия чувствительна к наличию выбросов.
# - Независимость — предполагает, что независимые переменные в модели не коррелируют и не связаны друг с другом. Это предположение необходимо для того, чтобы линейная регрессия была верной.
# - Нет мультиколлинеарности — это еще одно слово, которое обманчиво прямолинейно. Он описывает статистическое явление, когда два или более предикторов в модели множественной регрессии сильно коррелируют друг с другом. Таким образом, предположение об отсутствии мультиколлинеарности утверждает, что не должно быть никакой связи между предикторами в модели множественной линейной регрессии.
